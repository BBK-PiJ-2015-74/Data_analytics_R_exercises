---
title: "Coursework 4: Support Vector Machines, Logistic Regression revisited, Clustering and Principal Components Analyis""
author: "Lucie Burgess, 13109772"
date: "Saturday 12th December 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Q1. Support Vector Machines. 

This problem uses the OJ dataset which is part of the ISLR package.

##### (a) Create a training set which contains a random sample of 800 observations, and a test set containing the remaining observations.

```{r question_1a}
library(ISLR)
#?OJ
set.seed(123)
train<-sample(1:nrow(OJ), 800)
OJ.train=OJ[train,]
OJ.test=OJ[-train,]
```

##### (b) Fit a support vector classifier to the training data using cost=0.01, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained. 

```{r question_1b}
# install.packages("e1071")
library(e1071)
# install.packages("LiblineaR")
library(LiblineaR)

svm.linear=svm(Purchase~.,data=OJ.train,kernel="linear",cost=0.01,scale=FALSE)
summary(svm.linear)
```

Result: a linear kernel (model/ function) is used, i.e. a linear hyperplane separating the two sets of observations.  
Gamma is a parameter for all kernels except linear. The default is 1/(data dimension), 1/18.  
623 support vectors have been fitted to the data, 311 in one class (CH) and 312 in the other (MM).
There are two classes of observation i.e. two types of choice of orange juice - CH and MM.

###### (c) Calculate the training error rate and the test error rate

```{r question1c_train}

svm.predict.train=predict(svm.linear,OJ.train,type="class")
summary(svm.predict.train)
table(svm.predict.train,OJ.train$Purchase)
trainErrorRate=(53+123)/800
trainErrorRate
```
22% of OJ purchase observations incorrectly classified using SVM approach with this cost level.

```{r question1c_test}

svm.predict.test=predict(svm.linear,OJ.test,type="class")
summary(svm.predict.test)
table(svm.predict.test,OJ.test$Purchase)
testErrorRate=(13+45)/270
testErrorRate 
```
21.5% OJ purchase observations incorrectly classified using SVM approach with this cost level.

##### (d) Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10.

The e1071 library includes a built-in function, tune(), to perform cross-validation.  
By default, tune() performs ten-fold cross-validation on a set of models of interest.  
One of the parameters to be passed in is the range of values of the cost parameter.

```{r question_1d}

set.seed(1)
tune.out=tune(svm,Purchase~.,data=OJ.train,kernel="linear",ranges = list(cost=c(0.01,0.1,0.2,0.3,0.4,0.5,1,5,10)))
summary(tune.out)
```
Optimal value of cost is given by the lowest error rate on the training data = 10.0

##### (e) Compute the training and test error rates using this new value for cost.

Using value of 10:

```{r question_1e_train}

svmfit.tune=svm(Purchase~.,data=OJ.train,kernel="linear",cost=tune.out$best.parameter$cost,scale=FALSE)
summary(svmfit.tune)

svm.predict.train=predict(svmfit.tune,OJ.train,type="class")
summary(svm.predict.train) 
table(svm.predict.train,OJ.train$Purchase)
trainErrorRate=(57+77)/800
trainErrorRate
```
16.8% OJ purchase observations incorrectly classified using SVM approach with tuned cost = 0.3

```{r question_1e_test}

svm.predict.test=predict(svmfit.tune,OJ.test,type="class")
summary(svm.predict.test)
table(svm.predict.test,OJ.test$Purchase)
testErrorRate=(22+24)/270
testErrorRate 
```
17.0% OJ purchase observations incorrectly classified using SVM approach with this cost level

Therefore clearly showing that a larger value of cost gives a smaller number of support vectors, a narrower margin and a better result (i.e. a lower test error rate).

##### (f) Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for gamma.

```{r question_1f_radial}
svm.radial=svm(Purchase~.,data=OJ.train,kernel="radial",cost=0.01,scale=FALSE) 
summary(svm.radial)
```

```{r question_1f_train}
svm.predict.train=predict(svm.radial,OJ.train)
summary(svm.predict.train)
table(svm.predict.train,OJ.train$Purchase)

trainErrorRate=(0+316)/800
trainErrorRate
```

Predicts all values as CH using cost = 0.01  
39.5% OJ purchase observations incorrectly classified using SVM approach with this value of cost

```{r question_1f_test}

svm.predict.test=predict(svm.radial,OJ.test,type="class")
summary(svm.predict.test) 
table(svm.predict.test,OJ.test$Purchase) 
testErrorRate=(0+101)/270
testErrorRate
``` 
37.4% OJ purchase observations incorrectly classified using SVM approach with this cost level

```{r question_1f_tune}
set.seed(2)
tune.out=tune(svm,Purchase~.,data=OJ.train,kernel="radial",ranges = list(cost=c(0.01,0.1,0.2,0.3,0.4,0.5,1,5,10)))
summary(tune.out)

svm.radial.tune=svm(Purchase~.,data=OJ.train,kernel="radial",cost=tune.out$best.parameter$cost,scale=FALSE)
summary(svm.radial.tune)

svm.predict.train=predict(svm.radial.tune,OJ.train,type="class")
summary(svm.predict.train)
table(svm.predict.train,OJ.train$Purchase)
trainErrorRate=(32+254)/800
trainErrorRate
```
35.8% OJ purchase observations incorrectly classified using SVM approach with this cost level

```{r question 1f_tunetest}

svm.predict.test=predict(svm.radial.tune,OJ.test,type="class")
summary(svm.predict.test)
table(svm.predict.test,OJ.test$Purchase)
testErrorRate=(8+86)/270
testErrorRate
```
34.8% OJ purchase observations incorrectly classified using SVM approach with this cost level


##### (g) Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set degree=2.

```{r question_1g_polynomial}

svm.polynomial=svm(Purchase~.,data=OJ.train,kernel="polynomial",degree=2,cost=0.01,scale=FALSE) 
summary(svm.polynomial)
```
This time we use 341 support vectors to fit the data, 170 in one class and 171 in the other

```{r question_1g_train}

svm.predict.train=predict(svm.polynomial,OJ.train)
summary(svm.predict.train)
table(svm.predict.train,OJ.train$Purchase)
trainErrorRate=(78+57)/800
trainErrorRate
```
16.9% OJ purchase observations incorrectly classified using SVM approach with this cost level

```{r question_1g_test}

svm.predict.test=predict(svm.polynomial,OJ.test,type="class")
summary(svm.predict.test) 
table(svm.predict.test,OJ.test$Purchase)
testErrorRate=(25+19)/270
testErrorRate
```
16.3% OJ purchase observations incorrectly classified using SVM approach with this cost level

```{r question_1g_tune}

set.seed(3)
tune.out=tune(svm,Purchase~.,data=OJ.train,kernel="polynomial",degree=2,ranges = list(cost=c(0.01,0.1,0.2,0.3,0.4,0.5,1,5,10)))
summary(tune.out)

svm.polynomial.tune=svm(Purchase~.,data=OJ.train,kernel="polynomial",degree=2,cost=tune.out$best.parameter$cost,scale=FALSE)
summary(svm.polynomial.tune)

svm.predict.train=predict(svm.polynomial.tune,OJ.train,type="class")
summary(svm.predict.train) 
table(svm.predict.train,OJ.train$Purchase) 
trainErrorRate=(82+53)/800
trainErrorRate
```
16.9% OJ purchase observations incorrectly classified using SVM approach with this cost level

```{r question_1g_tunetest}

svm.predict.test=predict(svm.polynomial.tune,OJ.test,type="class")
summary(svm.predict.test)
table(svm.predict.test,OJ.test$Purchase)
testErrorRate=(28+17)/270
testErrorRate
```
16.7% OJ purchase observations incorrectly classified using SVM approach with this cost level

Comments: Both the linear and polynomial model (with degree = 2) give similar results, but the polynomial model performs slightly better on the test data. The radial kernel model gives poor results by comparison.


## Q2. SVM and logistic regression

We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary. We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features.

##### (a) Generate a data set with n = 500 and p = 2, such that the observations belong to two classes with a quadratic decision boundary between them.

```{r question_2a_setup}

?runif
set.seed(1)
x1=runif(500)-0.5
set.seed(2)
x2=runif(500)-0.5
y=1*(x1^2-x2^2>0)
```
Leaving off the 1* off the final function gives a vector containing values of FALSE/TRUE.
y just gives the class of the data: When x1^2-x2^2 > 0, y=1, otherwise y=0.

#####(b) Plot the observations, coloured according to their class labels. Your plot should display
# x1 on the x-axis and x2 on the y-axis.

```{r question_2b}

x=data.frame(x1,x2)
palette()
plot(x1,x2,col=ifelse(y==0,"blue","green"),pch=ifelse(y==0,"o","x"),xlab="X1",ylab="X2")
```

plot(x,col=(4-y)) # The data is (x1,x2,y). When x1^2-x2^2 > 0, y=1, otherwise y=0.
When y is 0, col = 4 = blue
When y = 1, col = 3 = green

#####(c) Fit a logistic regression model to the data, using X1 and X2 as predictors.

```{r question_2c}

data.train=data.frame(x1,x2,y)
glm.fit=glm(y~x1+x2,data=data.train,family="binomial") 
summary(glm.fit)
```

#####(d) Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations. The decision boundary should be linear

```{r question_2d, echo = TRUE}

glm.probs=predict(glm.fit,data=data.train,type="response")

# Choose y = 1 if glm.predict>0.47 - just above the mean and y = 0 if glm.predict < 0.47, as glm.predict will give a series of probabilities between 0 and 1.

mean(glm.probs)
glm.predict=rep("0",500)
glm.predict[glm.probs>0.47]=1
glm.predict[1:20]

plot(x1,x2,col=ifelse(glm.predict==0,"blue","green"),pch=ifelse(glm.predict==0,"o","+"),xlab="X1",ylab="X2")

table(glm.predict,y)
trainErr=(88+136)/500
trainErr
```

The chart shows very obviously a linear decision boundary.
Training error is 44.8% for the linear logistic regression model.

#####(e) Now fit a logistic regression model to the data using non-linear functions of X1 and X2 as predictors e.g. x1*x1, x1*x2, log(x2) or log(x1)

```{r question_2e}
glm.fit2=glm(y~ poly(x1, 2) + poly(x2, 2) + (x1 * x2), data=data.train, family="binomial") 
summary(glm.fit2)
```


#### (f) Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, according to the predicted class labels. The decision boundary should be obviously non-linear.

```{r question_2f}

glm.probs2=predict(glm.fit2,data=data.train,type="response")
glm.predict2=rep("0",500)
mean(glm.probs2)
glm.predict2[glm.probs2>0.47]=1

plot(x1,x2,col=ifelse(glm.predict2==0,"blue","green"),pch=ifelse(glm.predict2==0,"o","+"),xlab="X1",ylab="X2")

table(glm.predict2,y)
trainErr2=(0)/500
trainErr2 # amazingly zero!
```

Using a logistic regression model with non-linear features of x1 and x2 as predictors gives a surprisingly excellent model!

Now try using log(x1) and log(x2) as the response:

```{r question_2f_cont}

glm.fit3=glm(y~log(x1)+log(x2),data=data.train,family="binomial") 
summary(glm.fit3)

glm.probs3=predict(glm.fit3,data=data.train,type="response")
glm.predict3=rep("0",500)
mean(glm.probs3)
glm.predict3[glm.probs3>0.51]=1

plot(x1,x2,col=ifelse(glm.predict3==0,"blue","green"),pch=ifelse(glm.predict3==0,"o","+"),xlab="X1",ylab="X2")

table(glm.predict3,y)
trainErr3=(138+120)/500
trainErr3 
```
51.6% training error for the linear logistic regression using logx1+logx2 makes a worse fit


##### (g) Now fit a support vector classifier to the data with x1 and x2 as predictors. Obtain a class prediction for each training observation. Plot the observations, coloured according to the predicted class labels.

```{r question_1g}

library(e1071)
library(LiblineaR)
svm.linear=svm(y~x1+x2,data=data.train,kernel="linear",cost=0.01,scale=FALSE)
summary(svm.linear)
svm.probs=predict(svm.linear,data=trainingdata,type="class")
svm.probs[1:20]

svm.predict=rep("0",500)
mean(svm.probs)
svm.predict[svm.probs>0.1]=1
svm.predict[1:20]

plot(x1,x2,col=ifelse(svm.predict==0,"blue","green"),pch=ifelse(svm.predict==0,"o","+"),xlab="X1",ylab="X2")
table(svm.predict,y)
trainError=(64+127)/500
trainError
```

Comment: using a linear kernel for the SVM gives a linear decision boundary - obviously not a very accurate result - and a training error rate of 38.2%.

Now trying using a polynomial kernel for the SVM:

```{r question_1g_poly}

svm.poly=svm(y~x1+x2,data=data.train,kernel="polynomial",degree=2,cost=0.01,scale=FALSE)
summary(svm.poly)
svm.probs=predict(svm.poly,data=data.train,type="class")
svm.probs[1:20]

svm.predict=rep("0",500)
mean(svm.probs)
svm.predict[svm.probs>mean(svm.probs)]=1
svm.predict[1:20]

plot(x1,x2,col=ifelse(svm.predict==0,"blue","green"),pch=ifelse(svm.predict==0,"o","+"),xlab="X1",ylab="X2")
table(svm.predict,y)
trainError=(6+16)/500
trainError
```
Amazingly good result using the SVM approach with the decision boundary at the mean of the predictions - gives a training error rate of only 4.4%  
Changing the value of the decision boundary makes a big difference to the results and the shape of the curve.

Both SVM using a polynomial kernel and linear logistic regression using non-linear features give very similar, and very reliable results. The SVM model could be tuned further to an optimal level of cost.



